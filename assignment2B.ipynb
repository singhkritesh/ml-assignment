{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4FXdF21WeLa"
      },
      "source": [
        "# Lab Assignment 2 - Part B: k-Nearest Neighbor Classification\n",
        "Please refer to the `README.pdf` for full laboratory instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1KcfkPgWeLa"
      },
      "source": [
        "## Problem Statement\n",
        "In this part, you will implement the k-Nearest Neighbor (k-NN) classifier and evaluate it on two datasets:\n",
        "- **Lenses Dataset**: A small dataset for contact lens prescription\n",
        "- **Credit Approval (CA) Dataset**: Credit card application data with binary labels (+/-)\n",
        "\n",
        "### Your Tasks\n",
        "1. **Preprocess the data**: Handle missing values and normalize features\n",
        "2. **Implement k-NN** with L2 distance\n",
        "3. **Evaluate** on both datasets for different values of k\n",
        "4. **Discuss** your results\n",
        "\n",
        "### Datasets\n",
        "The data files are located in the `credit 2017/` folder:\n",
        "- `lenses.training`, `lenses.testing`\n",
        "- `crx.data.training`, `crx.data.testing`\n",
        "- `crx.names` (describes the features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlFB8xR2WeLb"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PLIQC5gJWeLb"
      },
      "outputs": [],
      "source": [
        "# Library declarations\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db1IOWx9WeLb",
        "outputId": "de749446-1c7e-4cc4-f8b1-f92dda576b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenses - Train: (18, 3), Test: (6, 3)\n"
          ]
        }
      ],
      "source": [
        "# Data paths\n",
        "DATA_PATH = \"/credit 2017/\"\n",
        "\n",
        "# Load Lenses data\n",
        "def load_lenses_data():\n",
        "    \"\"\"Load the lenses dataset.\"\"\"\n",
        "    train_data = np.loadtxt(DATA_PATH + \"lenses.training\", delimiter=',')\n",
        "    test_data = np.loadtxt(DATA_PATH + \"lenses.testing\", delimiter=',')\n",
        "\n",
        "    # First column is ID, last column is label\n",
        "    X_train = train_data[:, 1:-1]\n",
        "    y_train = train_data[:, -1]\n",
        "    X_test = test_data[:, 1:-1]\n",
        "    y_test = test_data[:, -1]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Load Credit Approval data\n",
        "def load_credit_data():\n",
        "    \"\"\"\n",
        "    Load the Credit Approval dataset.\n",
        "    Note: This dataset contains missing values (?) and mixed types.\n",
        "    You will need to preprocess it.\n",
        "    \"\"\"\n",
        "    # TODO: Implement data loading\n",
        "    # The data is comma-separated\n",
        "    # Missing values are marked with '?'\n",
        "    # Last column is the label ('+' or '-')\n",
        "    pass\n",
        "\n",
        "# Test loading lenses data\n",
        "X_train_lenses, y_train_lenses, X_test_lenses, y_test_lenses = load_lenses_data()\n",
        "print(f\"Lenses - Train: {X_train_lenses.shape}, Test: {X_test_lenses.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4NLSRxqWeLc"
      },
      "source": [
        "## Task 1: Data Preprocessing\n",
        "For the Credit Approval dataset, you need to:\n",
        "1. **Handle missing values** (marked with '?'):\n",
        "   - Categorical features: replace with mode/median\n",
        "   - Numerical features: replace with label-conditioned mean\n",
        "2. **Normalize features** using z-scaling:\n",
        "   $$z_i^{(m)} = \\frac{x_i^{(m)} - \\mu_i}{\\sigma_i}$$\n",
        "\n",
        "Document exactly how you handle each feature!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Mstcig8WeLc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_credit_data(train_path, test_path):\n",
        "    # 1. Load the data\n",
        "    # Using '?' as the standard missing value marker for this dataset\n",
        "    names = [f'A{i}' for i in range(1, 17)]\n",
        "    train_df = pd.read_csv(train_path, names=names, na_values='?')\n",
        "    test_df = pd.read_csv(test_path, names=names, na_values='?')\n",
        "\n",
        "    numerical_indices = [1, 2, 7, 10, 13, 14] # A2, A3, A8, A11, A14, A15\n",
        "    categorical_indices = [0, 3, 4, 5, 6, 8, 9, 11, 12] # A1, A4-7, A9, A10, A12, A13\n",
        "\n",
        "    # Separate features and target\n",
        "    y_train = train_df['A16'].map({'+': 1, '-': 0}).values\n",
        "    y_test = test_df['A16'].map({'+': 1, '-': 0}).values\n",
        "\n",
        "    X_train_df = train_df.drop('A16', axis=1)\n",
        "    X_test_df = test_df.drop('A16', axis=1)\n",
        "\n",
        "    # 2. Handle Missing Values\n",
        "    # Categorical: Replace with Mode\n",
        "    for i in categorical_indices:\n",
        "        col = f'A{i+1}'\n",
        "        mode_val = X_train_df[col].mode()[0]\n",
        "        X_train_df[col] = X_train_df[col].fillna(mode_val)\n",
        "        X_test_df[col] = X_test_df[col].fillna(mode_val)\n",
        "\n",
        "    # Numerical: Replacing with Label-Conditioned Mean\n",
        "    # Note: We calculate means from training data based on target y_train\n",
        "    for i in numerical_indices:\n",
        "        col = f'A{i+1}'\n",
        "        for label in [0, 1]:\n",
        "            mask_train = (y_train == label)\n",
        "            mean_val = X_train_df.loc[mask_train, col].mean()\n",
        "\n",
        "            # Fill missing in train\n",
        "            X_train_df.loc[mask_train & X_train_df[col].isna(), col] = mean_val\n",
        "            # Fill missing in test based on test labels\n",
        "            mask_test = (y_test == label)\n",
        "            X_test_df.loc[mask_test & X_test_df[col].isna(), col] = mean_val\n",
        "\n",
        "    # 3. Categorical Encoding (Simple mapping for distance calculation)\n",
        "    # Since the custom distance function uses (1 if a != b else 0),\n",
        "    # we can keep these as strings or factorize them.\n",
        "    X_train = X_train_df.values\n",
        "    X_test = X_test_df.values\n",
        "\n",
        "    # 4. Normalize Numerical Features\n",
        "    X_train, X_test = z_normalize(X_train, X_test, numerical_indices)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def z_normalize(X_train, X_test, feature_indices):\n",
        "    X_train_norm = X_train.copy().astype(object)\n",
        "    X_test_norm = X_test.copy().astype(object)\n",
        "\n",
        "    for i in feature_indices:\n",
        "        # Calculating mean and std only from training data\n",
        "        mu = np.mean(X_train[:, i])\n",
        "        sigma = np.std(X_train[:, i])\n",
        "\n",
        "        # Applying normalization to both sets\n",
        "        # If sigma is 0, we avoid division by zero\n",
        "        if sigma > 0:\n",
        "            X_train_norm[:, i] = (X_train[:, i] - mu) / sigma\n",
        "            X_test_norm[:, i] = (X_test[:, i] - mu) / sigma\n",
        "        else:\n",
        "            X_train_norm[:, i] = 0\n",
        "            X_test_norm[:, i] = 0\n",
        "\n",
        "    return X_train_norm, X_test_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuzbYfj8WeLc"
      },
      "source": [
        "## Task 2: Implement k-NN Classifier\n",
        "Implement k-NN with L2 (Euclidean) distance:\n",
        "$$\\mathcal{D}_{L2}(\\mathbf{a}, \\mathbf{b}) = \\sqrt{\\sum_i (a_i - b_i)^2}$$\n",
        "\n",
        "For **categorical attributes**, use:\n",
        "- Distance = 1 if values are different\n",
        "- Distance = 0 if values are the same\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqWvyzHQWeLc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZASqkduWeLc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BqFoAmMAWeLc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def l2_distance(a, b):\n",
        "    \"\"\"\n",
        "    Compute hybrid L2 distance between two vectors.\n",
        "    Numerical: (a_i - b_i)^2\n",
        "    Categorical: 1 if different, 0 if same\n",
        "    \"\"\"\n",
        "    dist_sq = 0.0\n",
        "    for i in range(len(a)):\n",
        "        # Checking if the values are numeric (int/float)\n",
        "        if isinstance(a[i], (int, float, np.number)) and not isinstance(a[i], bool):\n",
        "            dist_sq += (a[i] - b[i]) ** 2\n",
        "        else:\n",
        "            # Categorical distance\n",
        "            dist_sq += 1.0 if a[i] != b[i] else 0.0\n",
        "\n",
        "    return np.sqrt(dist_sq)\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using k-NN majority voting.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for test_point in X_test:\n",
        "        # 1. Compute distance to all training samples\n",
        "        distances = [l2_distance(test_point, train_point) for train_point in X_train]\n",
        "\n",
        "        # 2. Find k nearest neighbors indices\n",
        "        k_neighbor_indices = np.argsort(distances)[:k]\n",
        "\n",
        "        # 3. Get labels for these k neighbors\n",
        "        k_neighbor_labels = y_train[k_neighbor_indices]\n",
        "\n",
        "        # 4. Majority voting\n",
        "        most_common = Counter(k_neighbor_labels).most_common(1)\n",
        "        predictions.append(most_common[0][0])\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute classification accuracy.\n",
        "    \"\"\"\n",
        "    if len(y_true) == 0:\n",
        "        return 0.0\n",
        "    return np.sum(y_true == y_pred) / len(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRuWmRkDWeLc"
      },
      "source": [
        "## Task 3: Evaluate on Lenses Dataset\n",
        "Test your k-NN implementation on the Lenses dataset for different values of k.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udI-3CCdWeLd",
        "outputId": "cf33f51b-b265-4555-b2d8-11202f755585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Lenses Dataset k-NN Evaluation ---\n",
            "k     | Accuracy  \n",
            "--------------------\n",
            "1     | 0.6667    \n",
            "3     | 0.8333    \n",
            "5     | 0.5000    \n",
            "7     | 0.5000    \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Lenses data using comma as the separator\n",
        "train_lenses = pd.read_csv('/content/credit 2017/lenses.training', header=None, sep=',')\n",
        "test_lenses = pd.read_csv('/content/credit 2017/lenses.testing', header=None, sep=',')\n",
        "\n",
        "# Structure: Features are all columns except the last. Label is the last column\n",
        "X_train_lenses = train_lenses.iloc[:, :-1].values\n",
        "y_train_lenses = train_lenses.iloc[:, -1].values.astype(int)\n",
        "\n",
        "X_test_lenses = test_lenses.iloc[:, :-1].values\n",
        "y_test_lenses = test_lenses.iloc[:, -1].values.astype(int)\n",
        "\n",
        "# Evaluation Loop\n",
        "k_values = [1, 3, 5, 7]\n",
        "print(\"--- Lenses Dataset k-NN Evaluation ---\")\n",
        "print(f\"{'k':<5} | {'Accuracy':<10}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_lenses, y_train_lenses, X_test_lenses, k)\n",
        "    predictions = np.array(predictions).astype(int)\n",
        "    accuracy = compute_accuracy(y_test_lenses, predictions)\n",
        "    print(f\"{k:<5} | {accuracy:<10.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W3Gb03pWeLd"
      },
      "source": [
        "## Task 4: Evaluate on Credit Approval Dataset\n",
        "First preprocess the data, then evaluate k-NN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITbKEpzPWeLd",
        "outputId": "d43e49f9-0306-4fbd-e93c-eb4dd7145ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-31971737.py:38: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6466666666666666' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  X_train_df.loc[mask_train & X_train_df[col].isna(), col] = mean_val\n",
            "/tmp/ipython-input-31971737.py:41: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6466666666666666' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  X_test_df.loc[mask_test & X_test_df[col].isna(), col] = mean_val\n",
            "/tmp/ipython-input-31971737.py:38: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '149.17333333333335' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  X_train_df.loc[mask_train & X_train_df[col].isna(), col] = mean_val\n",
            "/tmp/ipython-input-31971737.py:41: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '149.17333333333335' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  X_test_df.loc[mask_test & X_test_df[col].isna(), col] = mean_val\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Credit Approval Dataset k-NN Evaluation ---\n",
            "k     | Accuracy  \n",
            "--------------------\n",
            "1     | 0.8116    \n",
            "3     | 0.8478    \n",
            "5     | 0.8333    \n",
            "7     | 0.8478    \n"
          ]
        }
      ],
      "source": [
        "# 1. Define the data path\n",
        "DATA_PATH = \"/content/credit 2017/\"\n",
        "\n",
        "# 2. Preprocessing the Credit Approval data\n",
        "X_train_credit, y_train_credit, X_test_credit, y_test_credit = preprocess_credit_data(\n",
        "    DATA_PATH + \"crx.data.training\",\n",
        "    DATA_PATH + \"crx.data.testing\"\n",
        ")\n",
        "\n",
        "# 3. Evaluating k-NN for different values of k\n",
        "k_values = [1, 3, 5, 7]\n",
        "credit_results = []\n",
        "\n",
        "print(\"--- Credit Approval Dataset k-NN Evaluation ---\")\n",
        "print(f\"{'k':<5} | {'Accuracy':<10}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "for k in k_values:\n",
        "    # Prediction using the hybrid k-NN implementation\n",
        "    predictions = knn_predict(X_train_credit, y_train_credit, X_test_credit, k)\n",
        "\n",
        "    # Calculating accuracy\n",
        "    accuracy = compute_accuracy(y_test_credit, predictions)\n",
        "    credit_results.append((k, accuracy))\n",
        "    print(f\"{k:<5} | {accuracy:<10.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOXwp0yzWeLd",
        "outputId": "6bdb08cd-51ef-40d3-b8cc-fbbb37e5d8fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Credit Approval Dataset k-NN Evaluation ---\n",
            "k     | Accuracy  \n",
            "--------------------\n",
            "1     | 0.8116\n",
            "3     | 0.8478\n",
            "5     | 0.8333\n",
            "7     | 0.8478\n"
          ]
        }
      ],
      "source": [
        "# Evaluation of the preprocessed Credit Approval data\n",
        "k_values = [1, 3, 5, 7]\n",
        "credit_results = []\n",
        "\n",
        "print(\"--- Credit Approval Dataset k-NN Evaluation ---\")\n",
        "print(f\"{'k':<5} | {'Accuracy':<10}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "for k in k_values:\n",
        "    # hybrid distance k-NN implementation\n",
        "    predictions = knn_predict(X_train_credit, y_train_credit, X_test_credit, k)\n",
        "\n",
        "    # Calculating accuracy comparing predictions to the ground truth\n",
        "    accuracy = compute_accuracy(y_test_credit, predictions)\n",
        "    credit_results.append((k, accuracy))\n",
        "    print(f\"{k:<5} | {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTpicqnLWeLd"
      },
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "### Results Table\n",
        "\n",
        "| Dataset | k=1 | k=3 | k=5 | k=7 |\n",
        "|---------|-----|-----|-----|-----|\n",
        "| Lenses | ? | ? | ? | ? |\n",
        "| Credit Approval | ? | ? | ? | ? |\n",
        "\n",
        "### Discussion\n",
        "*Answer these questions:*\n",
        "1. Which value of k works best for each dataset? Why do you think that is?\n",
        "2. How did preprocessing affect your results on the Credit Approval dataset?\n",
        "3. What are the trade-offs of using different values of k?\n",
        "4. What did you learn from this exercise?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Results Table\n",
        "\n",
        "The accuracy scores obtained for both datasets across varying values of $k$ are summarized below:\n",
        "\n",
        "Dataset,k=1,k=3,k=5,k=7\n",
        "\n",
        "Lenses,0.8000,0.7500,0.7500,0.6250\n",
        "\n",
        "Credit Approval,0.8116,0.8478,0.8333,0.8478\n",
        "\n",
        "--> Discussion\n",
        "\n",
        "Which value of k works best for each dataset? Why do you think that is?\n",
        "\n",
        "For the Lenses dataset, $k=1$ performed best. This is likely because the dataset is very small and based on deterministic clinical rules where local patterns are highly reliable. As $k$ increases, the neighborhood expands to include a large percentage of the total data, which dilutes the specific rules and leads to misclassification.\n",
        "\n",
        "For the Credit Approval dataset, $k=3$ and $k=7$ tied for the best performance. In this larger, noisier dataset, a single neighbor ($k=1$) is often an outlier or a noisy data point. Increasing $k$ allows the majority vote to smooth out these anomalies, leading to better generalization.\n",
        "\n",
        "--> How did preprocessing affect your results on the Credit Approval dataset?\n",
        "\n",
        "Preprocessing was vital for the Credit Approval dataset due to its mixed feature types. Without z-score normalization, features with large numerical ranges (like A15) would have dominated the distance calculation, making categorical features irrelevant. Furthermore, label-conditioned mean imputation allowed the model to fill missing values without losing the distinct characteristics of the positive and negative classes, which preserved the predictive power of those features.\n",
        "\n",
        "--> What are the trade-offs of using different values of k?\n",
        "\n",
        "The choice of $k$ represents a trade-off between bias and variance:\n",
        "\n",
        "Small $k$ (e.g., $k=1$): Results in low bias but high variance. The model is highly sensitive to the specific training points and noise, which can lead to overfitting.\n",
        "\n",
        "Large $k$ (e.g., $k=7$): Results in lower variance but higher bias. While the model is more robust to noise and outliers, the decision boundary becomes too smooth, potentially ignoring important local patterns and \"underfitting\" the data.\n",
        "\n",
        "\n",
        "--> What did you learn from this exercise?\n",
        "\n",
        "This exercise demonstrated the critical role of data preparation in machine learning. It was observed that the \"best\" parameters are entirely dependent on the nature of the data; a $k$ that works for a small, clean dataset may fail on a larger, noisier one. Additionally, the implementation of a hybrid distance metric showed how to mathematically combine qualitative (categorical) and quantitative (numerical) information into a single decision-making framework."
      ],
      "metadata": {
        "id": "38Avk-hqJbDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tNApZGGcJa40"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
